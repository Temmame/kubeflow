{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U kfp --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp  # the Pipelines SDK.  \n",
    "from kfp import compiler\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.components as comp\n",
    "from kfp.dsl.types import Integer, GCSPath, String\n",
    "import kfp.notebook\n",
    "import time\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the bucket \n",
    "\n",
    "Create it from User interface or from here by running cells below (if not exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'ribtdap-ds-aiplatform-kubeflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb gs://$BUCKET_NAME/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "client = kfp.Client(host='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "PROJECT_ID = ''\n",
    "BUCKET_URI = 'gs://' + BUCKET_NAME \n",
    "MODEL_NAME = 'model_aiplatform_kubeflow' \n",
    "MODEL_VERSION = MODEL_NAME + '_v1'\n",
    "PIPELINE_NAME = 'POC Pipeline'\n",
    "PIPELINE_DESCRIPTION = 'First POC'\n",
    "DATA_GCS_PATH = BUCKET_URI + '/data.csv'\n",
    "RUNTIME_VERSION = '2.1'\n",
    "PYTHON_VERSION = '3.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already built images \n",
    "IMG_BQ = 'https://raw.githubusercontent.com/kubeflow/pipelines/01a23ae8672d3b18e88adf3036071496aca3552d/components/gcp/bigquery/query/component.yaml'\n",
    "IMG_DEPLOY = 'https://raw.githubusercontent.com/kubeflow/pipelines/1.0.0/components/gcp/ml_engine/deploy/component.yaml'\n",
    "\n",
    "# Images built and push in Registry by the env.sh script\n",
    "IMG_PREPROCESS = 'gcr.io/ysance-datascience/poc-preprocess@sha256:7c4bd09d41b4c7957ba8c66d620938efc17fca1019e29b3c0793b380f0dae9cf'\n",
    "IMG_TRAIN = 'gcr.io/ysance-datascience/poc-train@sha256:4b93c1bb10b846099b84a9dec5698f959613d0f901c0c9f644e639dc029325d9'\n",
    "IMG_TEST = 'gcr.io/ysance-datascience/poc-test@sha256:eb59934ba45df9423f8b0713f013a4a53bec6f9fa42ce30f2b942a72d6952b8b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "Define a query and a download function that uses the BigQuery component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"\"\"\n",
    "        SELECT passenger_count, trip_distance , fare_amount as label\n",
    "        FROM `nyc-tlc.yellow.trips`\n",
    "        WHERE trip_distance > 0 AND fare_amount > 0\n",
    "        ORDER BY rand()\n",
    "        LIMIT 1000\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigquery_query_op = comp.load_component_from_url(IMG_BQ)\n",
    "\n",
    "def download(project_id, data_gcs_path):\n",
    "\n",
    "    return bigquery_query_op(\n",
    "        query=QUERY,\n",
    "        project_id=PROJECT_ID,\n",
    "        output_gcs_path=DATA_GCS_PATH\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data\n",
    "\n",
    "This step will use the image pushed in the google registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_op(file_gcs_path, bucket_name=BUCKET_NAME):\n",
    "\n",
    "    return dsl.ContainerOp(\n",
    "        name='Preprocess data',\n",
    "        image=IMG_PREPROCESS,\n",
    "        arguments=[\n",
    "            '--file_gcs_path', file_gcs_path,\n",
    "            '--bucket_name', bucket_name\n",
    "        ],\n",
    "        file_outputs={\n",
    "            'path_train': '/app/df_train.csv',\n",
    "            'path_test': '/app/df_test.csv'\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "This step will use the image pushed in the google registry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_op(path_train, bucket_name=BUCKET_NAME):\n",
    "    \n",
    "    return dsl.ContainerOp(\n",
    "        name='Train model',\n",
    "        image=IMG_TRAIN,\n",
    "        arguments=[\n",
    "            '--path_train', path_train,\n",
    "            '--bucket_name', bucket_name\n",
    "        ],\n",
    "        file_outputs={\n",
    "            'path_model': '/app/model.pkl'\n",
    "\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data\n",
    "\n",
    "This step will use the image pushed in the google registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_op(path_test, path_model, bucket_name=BUCKET_NAME):\n",
    "\n",
    "    return dsl.ContainerOp(\n",
    "        name='Test model',\n",
    "        image=IMG_TEST,\n",
    "        arguments=[\n",
    "            '--path_test', path_test,\n",
    "            '--path_model', path_model,\n",
    "            '--bucket_name', bucket_name\n",
    "        ],\n",
    "        file_outputs={\n",
    "            'path_metrics': '/app/metrics.txt',\n",
    "            'path_pred': '/app/df_pred.csv'\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model\n",
    "\n",
    "This step will use an already built image by Google. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlengine_deploy_op = comp.load_component_from_url(IMG_DEPLOY)\n",
    "\n",
    "def deploy(\n",
    "    project_id,\n",
    "    model_uri,\n",
    "    model_id,\n",
    "    model_version,\n",
    "    runtime_version,\n",
    "    python_version\n",
    "):\n",
    "    \n",
    "    return mlengine_deploy_op(\n",
    "        model_uri=model_uri,\n",
    "        project_id=project_id, \n",
    "        model_id=model_id, \n",
    "        version_id=model_version, \n",
    "        runtime_version=runtime_version,\n",
    "        python_version=python_version,\n",
    "        replace_existing_version=True, \n",
    "        set_default=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    description=PIPELINE_DESCRIPTION\n",
    ")\n",
    "\n",
    "def pipeline(\n",
    "    project_id,\n",
    "    bucket_name,\n",
    "    data_gcs_path,\n",
    "    model_uri,\n",
    "    model_id,\n",
    "    model_version,\n",
    "    runtime_version,\n",
    "    python_version\n",
    "):      \n",
    "    download_task = download(project_id,\n",
    "                             data_gcs_path)\n",
    "    \n",
    "    preprocess_task = preprocess_op(file_gcs_path=data_gcs_path,\n",
    "                                    bucket_name=bucket_name).after(download_task)\n",
    "    \n",
    "    train_task = train_op(path_train=dsl.InputArgumentPath(preprocess_task.outputs['path_train']),\n",
    "                         bucket_name=bucket_name).after(preprocess_task)\n",
    "   \n",
    "    test_task = test_op(path_test=dsl.InputArgumentPath(preprocess_task.outputs['path_test']),\n",
    "                        path_model=dsl.InputArgumentPath(train_task.outputs['path_model']),\n",
    "                        bucket_name=bucket_name).after(train_task)\n",
    "    \n",
    "    \n",
    "    deploy_task = deploy(project_id=project_id,\n",
    "                         model_uri=model_uri,\n",
    "                         model_id=model_id,\n",
    "                         model_version=model_version,\n",
    "                         runtime_version=runtime_version,\n",
    "                         python_version=python_version).after(train_task)\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline for execution\n",
    "\n",
    "This will : \n",
    "- create tar.gz file that contains the whole pipeline that you can share with anyone to reproduce the work\n",
    "- then it will run the pipeline according to the differents parameters in input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = client.create_experiment(name='POC Experiment')\n",
    "compiler.Compiler().compile(pipeline, 'poc_pipeline.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.run_pipeline(exp.id, \n",
    "                          job_name='POC Loreal', \n",
    "                          pipeline_package_path='poc_pipeline.tar.gz',\n",
    "                          params={\n",
    "                              'project_id': PROJECT_ID,\n",
    "                              'bucket_name': BUCKET_NAME,\n",
    "                              'data_gcs_path': DATA_GCS_PATH,\n",
    "                              'model_uri': BUCKET_URI + '/trained_model/' ,\n",
    "                              'model_id': MODEL_NAME,\n",
    "                              'model_version': MODEL_VERSION,\n",
    "                              'runtime_version': RUNTIME_VERSION,\n",
    "                              'python_version': PYTHON_VERSION\n",
    "                         })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the deployed model to predict (online prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict from shell\n",
    "\n",
    "Check README file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict from Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "\n",
    "def predict_json(project, model, instances, version=None):\n",
    "    \"\"\"Send json data to a deployed model for prediction.\n",
    "\n",
    "    Args:\n",
    "        project (str): project where the Cloud ML Engine Model is deployed.\n",
    "        model (str): model name.\n",
    "        instances ([Mapping[str: Any]]): Keys should be the names of Tensors\n",
    "            your deployed model expects as inputs. Values should be datatypes\n",
    "            convertible to Tensors, or (potentially nested) lists of datatypes\n",
    "            convertible to tensors.\n",
    "        version: str, version of the model to target.\n",
    "    Returns:\n",
    "        Mapping[str: any]: dictionary of prediction results defined by the\n",
    "            model.\n",
    "    \"\"\"\n",
    "    # Create the ML Engine service object.\n",
    "    # To authenticate set the environment variable\n",
    "    # GOOGLE_APPLICATION_CREDENTIALS=<path_to_service_account_file>\n",
    "    service = googleapiclient.discovery.build('ml', 'v1')\n",
    "    name = 'projects/{}/models/{}'.format(project, model)\n",
    "\n",
    "    if version is not None:\n",
    "        name += '/versions/{}'.format(version)\n",
    "\n",
    "    response = service.projects().predict(\n",
    "        name=name,\n",
    "        body={'instances': instances}\n",
    "    ).execute()\n",
    "\n",
    "    if 'error' in response:\n",
    "        raise RuntimeError(response['error'])\n",
    "\n",
    "    return response['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances= [[1, 14], [3, 14], [1, 10]]\n",
    "result = predict_json(PROJECT_ID, MODEL_NAME, instances, version=MODEL_VERSION)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean models\n",
    "\n",
    "You can do it from this notebook, from the terminal by running commands below OR from the user interface `AI Platform -> models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud ai-platform versions delete $MODEL_VERSION --model $MODEL_NAME --quiet\n",
    "#!gcloud ai-platform models delete $MODEL_NAME --quiet"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
